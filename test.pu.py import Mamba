from Mamba_pde.model.py import Mamba


model = Mamba(d_model=128, n_layers=4, d_state=16,
              expand=2, vocab_size=1000)

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# If GPU available, device='cuda', otherwise device='cpu'

model = model.to(device)
# Move model to GPU (if available)

# Create random input
batch_size = 2
seq_len = 32
input_ids = torch.randint(0, 1000, (batch_size, seq_len)).to(device)
# torch.randint(low, high, size) creates random integers
# .to(device) moves to same device as model

# Forward pass without gradient tracking
with torch.no_grad():
    output = model(input_ids)

print(f"Input shape: {input_ids.shape}")   # (2, 32)
print(f"Output shape: {output.shape}")      # (2, 32, 1000)
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")